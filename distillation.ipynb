{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/iws/gstrau2/CSE481N_Project/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets import Dataset, DatasetDict\n",
    "from typing import Dict, List\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda:1\"\n",
    "\n",
    "model.eval()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmptyFrameDict(\n",
    "    input_key: str,\n",
    "    output_key: str,\n",
    "    dataset_dict: DatasetDict\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    dataframe_dict = dict()\n",
    "\n",
    "    for curr_name, curr_dataset in dataset_dict.items():\n",
    "        next_dataframe = pd.DataFrame.from_dict({\n",
    "            input_key: curr_dataset[input_key],\n",
    "            output_key: [\"\"] * len(curr_dataset[input_key])\n",
    "        })\n",
    "\n",
    "        dataframe_dict[curr_name] = next_dataframe\n",
    "\n",
    "    return dataframe_dict\n",
    "\n",
    "def cacheFrame(\n",
    "    data_frame: pd.DataFrame,\n",
    "    cache_dir: Path,\n",
    "    cache_entry_name: str\n",
    ") -> None:\n",
    "    file_name = f\"{cache_entry_name}.cache.parquet\"\n",
    "    data_frame.to_parquet(\n",
    "        os.path.join(\n",
    "            cache_dir,\n",
    "            file_name\n",
    "        ),\n",
    "        engine = \"pyarrow\",\n",
    "        compression = None\n",
    "    )\n",
    "\n",
    "def cacheFrameDict(\n",
    "    cache_dir: Path,\n",
    "    dataframe_dict: Dict[str, pd.DataFrame],\n",
    "    prefix_name: str = None,\n",
    ") -> None:\n",
    "    for curr_name, curr_dataframe in dataframe_dict.items():\n",
    "        file_name = curr_name\n",
    "\n",
    "        if prefix_name is not None:\n",
    "            file_name = f\"{prefix_name}_{file_name}\"\n",
    "\n",
    "        cacheFrame(\n",
    "            curr_dataframe,\n",
    "            cache_dir,\n",
    "            file_name\n",
    "        )\n",
    "\n",
    "def loadFrameDict(\n",
    "    cache_dir: Path,\n",
    "    dataframe_names: List[str],\n",
    "    prefix_name: str = None,\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    dataframe_dict = dict()\n",
    "\n",
    "    for curr_name in dataframe_names:\n",
    "        file_name = f\"{curr_name}.cache.parquet\"\n",
    "\n",
    "        if prefix_name is not None:\n",
    "            file_name = f\"{prefix_name}_{file_name}\"\n",
    "\n",
    "        dataframe_dict[curr_name] = pd.read_parquet(\n",
    "            os.path.join(\n",
    "                cache_dir,\n",
    "                file_name\n",
    "            ),\n",
    "            engine = \"pyarrow\",\n",
    "        )\n",
    "\n",
    "    return dataframe_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Cells if Creating Fresh Frame Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_frame_dict = getEmptyFrameDict(\"article\", \"t5_large_output\", cnn_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cacheFrameDict(\"./cache\", cnn_frame_dict, \"cnn_dm_distill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUn Cell if Using Cached Frame Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_frame_dict = loadFrameDict(\"./cache\", cnn_dataset.keys(), \"cnn_dm_distill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/iws/gstrau2/CSE481N_Project/.venv/lib64/python3.9/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "100%|██████████| 17945/17945 [4:43:59<00:00,  1.05it/s]  \n",
      "/homes/iws/gstrau2/CSE481N_Project/.venv/lib64/python3.9/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "100%|██████████| 836/836 [50:01<00:00,  3.59s/it]\n",
      "100%|██████████| 719/719 [43:05<00:00,  3.60s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inferDataFrameDict(\n",
    "    dataframe_dict: Dict[str, pd.DataFrame],\n",
    "    batch_size: int,\n",
    "    input_key: str = \"article\",\n",
    "    output_key: str = \"t5_large_output\",\n",
    "    prefix: str = \"summarize: \",\n",
    "    max_input_length: int = 512,\n",
    "    max_output_length: int = 512,\n",
    "    cache_location: str = None,\n",
    "    dataset_name: str = None,\n",
    "    batches_per_cache_write: int = None\n",
    ") -> None:\n",
    "    using_cache = all(x is not None for x in [cache_location, dataset_name, batches_per_cache_write])\n",
    "    model.eval()\n",
    "\n",
    "    for curr_name, curr_dataframe in dataframe_dict.items():\n",
    "        chunks_iter = np.array_split(curr_dataframe, (len(curr_dataframe) // batch_size) + 1)\n",
    "        row_counter = 0\n",
    "        \n",
    "        for chunk_idx, curr_chunk in enumerate(tqdm(chunks_iter)):\n",
    "            curr_chunk_inputs = list(curr_chunk[input_key])\n",
    "            curr_chunk_outputs = list(curr_chunk[output_key])\n",
    "            is_cached = all(x != \"\" for x in curr_chunk_outputs)\n",
    "\n",
    "            if is_cached:\n",
    "                row_counter += len(curr_chunk_outputs)\n",
    "                continue\n",
    "\n",
    "            inputs = [prefix + doc for doc in curr_chunk_inputs]\n",
    "            input_ids = tokenizer(\n",
    "                inputs, \n",
    "                return_tensors = \"pt\",\n",
    "                max_length = max_input_length,\n",
    "                truncation = True,\n",
    "                padding = True,\n",
    "            ).input_ids.to(DEVICE)\n",
    "\n",
    "            outputs = model.generate(input_ids, max_new_tokens = max_output_length)\n",
    "            outputs.to(\"cpu\")\n",
    "            decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens = True)\n",
    "\n",
    "            out_column_index = curr_dataframe.columns.get_loc(output_key)\n",
    "            end_row_index = row_counter + len(decoded_output)\n",
    "            curr_dataframe.iloc[row_counter : end_row_index, out_column_index] = decoded_output\n",
    "            row_counter += len(curr_chunk_outputs)\n",
    "\n",
    "            if (((chunk_idx + 1) % batches_per_cache_write) == 0) and using_cache:\n",
    "                cacheFrame(\n",
    "                    curr_dataframe,\n",
    "                    cache_location,\n",
    "                    f\"{dataset_name}_{curr_name}\"\n",
    "                )\n",
    "            \n",
    "        cacheFrame(\n",
    "            curr_dataframe,\n",
    "            cache_location,\n",
    "            f\"{dataset_name}_{curr_name}\"\n",
    "        )\n",
    "\n",
    "inferDataFrameDict(\n",
    "    cnn_frame_dict,\n",
    "    16,\n",
    "    cache_location = \"./cache\",\n",
    "    dataset_name = \"cnn_dm_distill\",\n",
    "    batches_per_cache_write = 64\n",
    ")\n",
    "\n",
    "# Move model out of VRAM (so NLPG admins don't get mad at us)\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>t5_large_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(CNN)Share, and your gift will be multiplied. ...</td>\n",
       "      <td>Zully broussard gave one of her kidneys to a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(CNN)On the 6th of April 1996, San Jose Clash ...</td>\n",
       "      <td>MLS is celebrating its 20th season. the number...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(CNN)French striker Bafetimbi Gomis, who has a...</td>\n",
       "      <td>french striker bafetimbi gomis says he is now ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(CNN)It was an act of frustration perhaps more...</td>\n",
       "      <td>Rory McIlroy launches 3-iron into lake after d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(CNN)A Pennsylvania community is pulling toget...</td>\n",
       "      <td>the parents of cayman naib have been communica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13363</th>\n",
       "      <td>It is the dream of many young children, the ch...</td>\n",
       "      <td>the Wild west town in valley center, californi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13364</th>\n",
       "      <td>It’s the type of encounter that can send panic...</td>\n",
       "      <td>photographer Graham hewer captured the jaw-dro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13365</th>\n",
       "      <td>A group of tourists to the Bahamas enjoyed one...</td>\n",
       "      <td>the pigs were filmed on a boat on the island o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13366</th>\n",
       "      <td>Pippa Middleton bundled up against the London ...</td>\n",
       "      <td>the brunette is back in London following news ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13367</th>\n",
       "      <td>A teacher and wrestling coach has been charged...</td>\n",
       "      <td>Megan Blair baker, 25, has been accused of sex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13368 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 article  \\\n",
       "0      (CNN)Share, and your gift will be multiplied. ...   \n",
       "1      (CNN)On the 6th of April 1996, San Jose Clash ...   \n",
       "2      (CNN)French striker Bafetimbi Gomis, who has a...   \n",
       "3      (CNN)It was an act of frustration perhaps more...   \n",
       "4      (CNN)A Pennsylvania community is pulling toget...   \n",
       "...                                                  ...   \n",
       "13363  It is the dream of many young children, the ch...   \n",
       "13364  It’s the type of encounter that can send panic...   \n",
       "13365  A group of tourists to the Bahamas enjoyed one...   \n",
       "13366  Pippa Middleton bundled up against the London ...   \n",
       "13367  A teacher and wrestling coach has been charged...   \n",
       "\n",
       "                                         t5_large_output  \n",
       "0      Zully broussard gave one of her kidneys to a s...  \n",
       "1      MLS is celebrating its 20th season. the number...  \n",
       "2      french striker bafetimbi gomis says he is now ...  \n",
       "3      Rory McIlroy launches 3-iron into lake after d...  \n",
       "4      the parents of cayman naib have been communica...  \n",
       "...                                                  ...  \n",
       "13363  the Wild west town in valley center, californi...  \n",
       "13364  photographer Graham hewer captured the jaw-dro...  \n",
       "13365  the pigs were filmed on a boat on the island o...  \n",
       "13366  the brunette is back in London following news ...  \n",
       "13367  Megan Blair baker, 25, has been accused of sex...  \n",
       "\n",
       "[13368 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_frame_dict[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 287113 examples [00:22, 12878.38 examples/s]\n",
      "Generating validation split: 13368 examples [00:00, 23574.17 examples/s]\n",
      "Generating test split: 11490 examples [00:00, 24219.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face doesn't provide an easy way to load a DatasetDict from Pandas,\n",
    "# so loadDatasetFromCachedDataframe pulls the latest cache entry instead\n",
    "def loadDatasetFromCachedDataframe(\n",
    "    cache_dir: Path,\n",
    "    dataframe_names: List[str],\n",
    "    prefix_name: str = None,\n",
    ") -> DatasetDict:\n",
    "    file_dict = dict()\n",
    "\n",
    "    for curr_name in dataframe_names:\n",
    "        file_name = f\"{curr_name}.cache.parquet\"\n",
    "\n",
    "        if prefix_name is not None:\n",
    "            file_name = f\"{prefix_name}_{file_name}\"\n",
    "\n",
    "        file_dict[curr_name] = os.path.join(cache_dir, file_name)\n",
    "\n",
    "    return DatasetDict.from_parquet(file_dict)\n",
    "\n",
    "cnn_distill_dataset = loadDatasetFromCachedDataframe(\"./cache\", cnn_dataset.keys(), \"cnn_dm_distill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'highlights', 'id'],\n",
       "    num_rows: 287113\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for curr_name, curr_dataset in cnn_distill_dataset.items():\n",
    "    curr_dataset = curr_dataset.add_column(\n",
    "        \"highlights\",\n",
    "        cnn_dataset[curr_name][\"highlights\"]\n",
    "    )\n",
    "\n",
    "    curr_dataset = curr_dataset.add_column(\n",
    "        \"id\",\n",
    "        cnn_dataset[curr_name][\"id\"]\n",
    "    )\n",
    "\n",
    "    cnn_distill_dataset[curr_name] = curr_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': 'LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.',\n",
       " 't5_large_output': \"Harry Potter star Daniel Radcliffe turns 18 on monday. the young actor says he has no plans to fritter his cash away. details of how he'll mark his landmark birthday are under wraps.\",\n",
       " 'highlights': \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\",\n",
       " 'id': '42c027e4ff9730fbb3de84c1af0d2c506e41c3e4'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_distill_dataset.push_to_hub(\n",
    "    \"lilferrit/cnn_dailymail_t5_distillation\",\n",
    "    config_name = \"3.0.0\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_swag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
